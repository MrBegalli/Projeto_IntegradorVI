# Treinamento DQN - Super Trunfo RL Bot

Este diret√≥rio cont√©m o script de treinamento para o RL Bot usando Deep Q-Network (DQN).

## üìÅ Arquivos

- `train_dqn.py` - Script √∫nico de treinamento DQN

## üöÄ Como Usar

### Treinamento B√°sico

```bash
python train_dqn.py
```

Isso executar√° o treinamento com os par√¢metros padr√£o:
- 50.000 epis√≥dios
- Avalia√ß√£o a cada 1.000 epis√≥dios
- Salvamento a cada 5.000 epis√≥dios

### Treinamento Personalizado

```bash
python train_dqn.py --episodes 100000 --eval-interval 2000 --save-interval 10000
```

### Continuar Treinamento de Modelo Existente

```bash
python train_dqn.py --model ../data/dqn_model.pth --episodes 20000
```

## üéØ Par√¢metros

| Par√¢metro | Descri√ß√£o | Padr√£o |
|-----------|-----------|--------|
| `--episodes` | N√∫mero total de epis√≥dios de treinamento | 50000 |
| `--eval-interval` | Intervalo entre avalia√ß√µes de desempenho | 1000 |
| `--save-interval` | Intervalo entre salvamentos de checkpoint | 5000 |
| `--model` | Caminho para modelo pr√©-treinado (continuar treinamento) | None |

## üìä Processo de Treinamento

### Fases do Treinamento

O treinamento √© dividido em 3 fases progressivas:

1. **Fase Inicial (0-30% dos epis√≥dios)**
   - 70% dos jogos contra Facil_Bot
   - 30% dos jogos contra Medio_Bot
   - Foco em aprender estrat√©gias b√°sicas

2. **Fase Intermedi√°ria (30-60% dos epis√≥dios)**
   - 50% dos jogos contra Facil_Bot
   - 50% dos jogos contra Medio_Bot
   - Balanceamento entre aprendizado b√°sico e avan√ßado

3. **Fase Final (60-100% dos epis√≥dios)**
   - 30% dos jogos contra Facil_Bot
   - 70% dos jogos contra Medio_Bot
   - Foco em superar oponentes mais fortes

### Epsilon Decay

O par√¢metro epsilon controla a explora√ß√£o vs explota√ß√£o:

- **In√≠cio:** Œµ = 1.0 (explora√ß√£o m√°xima)
- **Fim:** Œµ = 0.05 (explota√ß√£o m√°xima)
- **Decay:** Linear ao longo dos epis√≥dios

### Sistema de Recompensas

| Evento | Recompensa (Facil_Bot) | Recompensa (Medio_Bot) |
|--------|------------------------|------------------------|
| Vit√≥ria na rodada | +3.0 | +5.0 |
| Derrota na rodada | -2.0 | -3.0 |
| Empate na rodada | -0.5 | -0.5 |
| Vit√≥ria no jogo | +10.0 | +15.0 |
| Derrota no jogo | -5.0 | -8.0 |

## üìà Monitoramento

### Logs

Os logs s√£o salvos em `../logs/training_dqn_YYYYMMDD_HHMMSS.log`

Exemplo de sa√≠da:
```
[2024-11-23 14:30:00] ================================================================================
[2024-11-23 14:30:00] Epis√≥dio 1,000/50,000
[2024-11-23 14:30:00]   Epsilon: 0.9800
[2024-11-23 14:30:00]   Recompensa m√©dia (√∫ltimos 100): 12.45
[2024-11-23 14:30:00]   Tamanho do buffer: 5000
[2024-11-23 14:30:00] 
[2024-11-23 14:30:00]   Avaliando desempenho (100 jogos por oponente)...
[2024-11-23 14:30:05]     vs Facil_Bot   :  85.0% vit√≥rias |  5.0% empates
[2024-11-23 14:30:10]     vs Medio_Bot   :  45.0% vit√≥rias | 10.0% empates üî• NOVO RECORDE!
[2024-11-23 14:30:10] ================================================================================
```

### M√©tricas Avaliadas

A cada `eval-interval` epis√≥dios, o script avalia:

1. **Recompensa M√©dia:** M√©dia das √∫ltimas 100 recompensas
2. **Taxa de Vit√≥ria:** Percentual de vit√≥rias contra cada oponente
3. **Taxa de Empate:** Percentual de empates
4. **Epsilon Atual:** N√≠vel de explora√ß√£o vs explota√ß√£o
5. **Tamanho do Buffer:** Quantidade de experi√™ncias armazenadas

### Arquivos Gerados

1. **Modelo Treinado:** `../data/dqn_model.pth`
   - Pesos da rede neural
   - Pode ser carregado para continuar treinamento ou infer√™ncia

2. **Hist√≥rico de Treinamento:** `../data/dqn_training_history.json`
   - Epis√≥dios executados
   - Recompensas m√©dias
   - Valores de epsilon
   - Taxas de vit√≥ria ao longo do tempo

Exemplo de hist√≥rico:
```json
{
  "episodes": [1000, 2000, 3000, ...],
  "avg_rewards": [12.45, 15.30, 18.20, ...],
  "epsilon_values": [0.98, 0.96, 0.94, ...],
  "win_rates": {
    "Facil_Bot": [0.85, 0.88, 0.90, ...],
    "Medio_Bot": [0.45, 0.48, 0.52, ...]
  }
}
```

## üéØ Metas de Desempenho

### Objetivos

| Oponente | Meta de Vit√≥ria | Excelente |
|----------|----------------|-----------|
| Facil_Bot | > 80% | > 90% |
| Medio_Bot | > 50% | > 60% |

### Tempo Estimado

- **50.000 epis√≥dios:** ~2-4 horas (dependendo do hardware)
- **100.000 epis√≥dios:** ~4-8 horas

## üîß Troubleshooting

### Problema: Treinamento muito lento

**Solu√ß√£o:**
- Verifique se PyTorch est√° usando GPU: `torch.cuda.is_available()`
- Reduza o n√∫mero de simula√ß√µes do MCTS Bot (edite `train_dqn.py`)
- Use menos epis√≥dios de avalia√ß√£o

### Problema: Taxa de vit√≥ria n√£o melhora

**Solu√ß√£o:**
- Aumente o n√∫mero de epis√≥dios
- Ajuste os hiperpar√¢metros (epsilon, alpha, gamma)
- Verifique se o sistema de recompensas est√° adequado

### Problema: Mem√≥ria insuficiente

**Solu√ß√£o:**
- Reduza o tamanho do buffer de replay (edite `rl_model.py`)
- Use batch size menor
- Feche outros programas

## üìö Refer√™ncias

- [Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)](https://arxiv.org/abs/1312.5602)
- [Human-level control through deep reinforcement learning (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)

## üí° Dicas

1. **Comece com poucos epis√≥dios** para testar se tudo est√° funcionando
2. **Monitore os logs** para identificar problemas cedo
3. **Salve checkpoints frequentemente** para n√£o perder progresso
4. **Experimente diferentes hiperpar√¢metros** para melhorar o desempenho
5. **Use GPU** se dispon√≠vel para acelerar o treinamento

---

**Boa sorte com o treinamento! üöÄ**
